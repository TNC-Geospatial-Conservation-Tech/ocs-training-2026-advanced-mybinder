{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3752e76c",
   "metadata": {},
   "source": [
    "# Lesson 3b. Access PRISM datasets and visualize climate data in 3d with `lexcube`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f55604",
   "metadata": {},
   "source": [
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3fs import S3FileSystem\n",
    "\n",
    "# Instantiate the filesystem for a public bucket\n",
    "s3 = S3FileSystem(anon=True)\n",
    "\n",
    "# Define the S3 URI (path)\n",
    "s3_uri = 's3://ocs-training-2026/advanced/'\n",
    "\n",
    "# List all files and subdirectories\n",
    "all_files_and_dirs = s3.ls(s3_uri)\n",
    "print(all_files_and_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9713d24c",
   "metadata": {},
   "source": [
    "### Step 2. Get a list of GeoTIFF files from an S3 folder \n",
    "\n",
    "To get a list of all .tif files inside a specific S3 \"folder\" (prefix), you need to use the **boto3** library to list all objects with the specified prefix and then filter the results client-side to keep only those ending with .tif. S3 does not support filtering by suffix on the service end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def list_tif_files_in_s3_folder(bucket_name, folder_prefix):\n",
    "    \"\"\"\n",
    "    Lists all .tif files within a specific folder prefix in an S3 bucket.\n",
    "\n",
    "    :param bucket_name: The name of the S3 bucket.\n",
    "    :param folder_prefix: The 'folder' path (prefix) within the bucket.\n",
    "    :return: A list of S3 object keys (file names) ending with '.tif'.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    tif_files = []\n",
    "\n",
    "    # Use a paginator to handle cases with more than 1000 objects\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=folder_prefix)\n",
    "\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                # Client-side filtering for the .tif suffix\n",
    "                if obj['Key'].lower().endswith('.tif') or obj['Key'].lower().endswith('.tiff'):\n",
    "                    tif_files.append(obj['Key'])\n",
    "                    \n",
    "    return tif_files\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Replace with your bucket name and folder prefix\n",
    "s3_bucket_name = 'ocs-training-2026'\n",
    "s3_folder_prefix = 'advanced/PRISM/unzipped/' # Ensure a trailing slash for a specific folder\n",
    "\n",
    "# Get the list of .tif files\n",
    "geotiff_files = list_tif_files_in_s3_folder(s3_bucket_name, s3_folder_prefix)\n",
    "\n",
    "# Print the list of files\n",
    "if geotiff_files:\n",
    "    print(f\"Found {len(geotiff_files)} .tif files in s3://{s3_bucket_name}/{s3_folder_prefix}:\")\n",
    "    for file_key in geotiff_files:\n",
    "        print(file_key)\n",
    "else:\n",
    "    print(f\"No .tif files found in s3://{s3_bucket_name}/{s3_folder_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f74dcc",
   "metadata": {},
   "source": [
    "### Step 3. Visualize Climate Data (2D)\n",
    "Let's start our big data viz journey by using some sample climate datasets in GeoTIFF format, downloaded from the [PRISM online repository](https://prism.oregonstate.edu/downloads/) by Oregon State University. If you are interested in how to programmatically download these data, please refer to [this blog post](https://medium.com/@mahyar.aboutalebi/how-to-download-process-and-visualize-climate-data-in-python-cfd6d8350322). To simplify things, we pre-downloaded the data for this training. In this step, we will read a raster file of the mean air temperature for the CONUS extent for July of 2025, replace -9999 values with NaN, create a mesh grid for latitude and longitude, and set up the titles, x-axis and y-axis labels, and color ramp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04298e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#s3_url = 's3://your-bucket-name/path/to/your/image.tif'\n",
    "s3_url = 's3://ocs-training-2026/advanced/PRISM/unzipped/prism_tmean_us_25m_202507.tif'\n",
    "\n",
    "# Open the raster file\n",
    "with rasterio.open(f'{s3_url}') as src:\n",
    "    # Read the temperature data from the first band\n",
    "    temp = src.read(1)\n",
    "    # Replace -9999 values with NaN\n",
    "    temp[temp == -9999] = np.nan\n",
    "    # Get the latitude and longitude coordinates\n",
    "    lon_min, lat_min, lon_max, lat_max = src.bounds\n",
    "    lat_res, lon_res = src.res\n",
    "    lats = np.arange(lat_min, lat_max, lat_res)\n",
    "    lons = np.arange(lon_min, lon_max, lon_res)\n",
    "    lons, lats = np.meshgrid(lons, lats)\n",
    "\n",
    "    # Set the minimum and maximum values of the color scale\n",
    "    vmin = 10\n",
    "    vmax = 30\n",
    "\n",
    "    # Plot the temperature data with a color ramp\n",
    "    cmap = plt.colormaps.get_cmap('jet')\n",
    "    cmap.set_bad('white')\n",
    "\n",
    "    # Set the size of the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Plot the temperature data with a color ramp\n",
    "    im = ax.imshow(temp, cmap=cmap, vmin=vmin, vmax=vmax, extent=[lons.min(), lons.max(), lats.min(), lats.max()])\n",
    "\n",
    "    # Add a colorbar with a smaller size\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "    # Set the x and y axis labels\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title('Temperature for July')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df825f",
   "metadata": {},
   "source": [
    "### Step 4. Create a Data Cube (3D) with Random Numbers using `Zarr`\n",
    "Let's take a step forward and move to multidimensional data. To create a cubic dataset with random numbers, we will import numpy, xarray, zarr, and pandas libraries. We will generate 720 latitudes from -90 to +90, 1440 longitudes from -180 to +180, and 365 days from 2023‚Äì01‚Äì01 to 2023‚Äì12‚Äì31. Next, we will create a 3D array with random numbers ranging from -20 to 30 and set time, latitude, and longitude. After that, we will convert the array to Xarray, save it in Zarr format, and label the data as \"air temperature\". The following lines do these steps for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb96b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import zarr # you don't need to run this as long as you have the zarr package installed\n",
    "\n",
    "# Set dimensions\n",
    "lat = np.linspace(-90, 90, 720)\n",
    "lon = np.linspace(-180, 180, 1440)\n",
    "time = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Create 3D array with random numbers\n",
    "data = np.random.uniform(low=-20, high=30, size=(len(time), len(lat), len(lon)))\n",
    "\n",
    "# Create xarray DataArray with coordinate labels\n",
    "data_array = xr.DataArray(data, coords={\"time\": time, \"lat\": lat, \"lon\": lon}, dims=[\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "# Use .to_dataset() function to convert the DataArray to a Dataset with the specified name\n",
    "dataset = data_array.to_dataset(name=\"air_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87f2fe",
   "metadata": {},
   "source": [
    "### Write the random numbers data cube as a `zarr` datasets to an AWS S3 bucket\n",
    "‚ö†Ô∏è **NOTE: THE FOLLOWING CODE BLOCK SAVES THE DATASET TO S3 AS A ZARR FILE. FOR SECURITY REASONS, WE HAVE REMOVE 'WRITE' ACCESS TO THE PUBLIC BUCKET USED IN THIS TRAINING. YOU CAN USE THIS CODE TO SAVE TO YOUR OWN S3 BUCKET WITHIN TNC's AWS ACCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 File System\n",
    "s3_out = s3fs.S3FileSystem() # Assumes credentials are set in environment/config\n",
    "\n",
    "# Create an S3Map store for the target location\n",
    "# Replace 'your-bucket-name' and 'path/to/your/file.zarr' with your actual S3 details\n",
    "# s3_uri = 's3://your-bucket-name/path/to/your/dataset.zarr'\n",
    "s3_uri = 's3://ocs-training-2026/advanced/PRISM/random_data.zarr'\n",
    "store = s3fs.S3Map(root=s3_uri, s3=s3_out, check=False)\n",
    "\n",
    "# Write the Dataset to the S3 store as a Zarr file\n",
    "# Use mode='w' to create a new store or overwrite an existing one\n",
    "dataset.to_zarr(store=store, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02c87d",
   "metadata": {},
   "source": [
    "### Read the random numbers data cube `zarr` dataset from an existing AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaff90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fsspec library is used under the hood to handle the S3 protocol and authentication\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "# Define the S3 URL of the Zarr dataset\n",
    "s3_uri = 's3://ocs-training-2026/advanced/random_data.zarr'\n",
    "\n",
    "# Open the dataset with anonymous access for public datasets\n",
    "# Here we are using the open_zarr function from the xarray library, which is a more direct and robust way to open Zarr datasets.\n",
    "ds = xr.open_zarr(\n",
    "    s3_uri,\n",
    "    consolidated=True, # Optional: use if metadata is consolidated for faster opening\n",
    "    storage_options={'anon': True} # Optional: use anonymous access for public datasets\n",
    ")\n",
    "\n",
    "# Alternatively, if you can also use the open_dataset function from the xarray library\n",
    "#ds = xr.open_dataset(\"climate.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedc9aa",
   "metadata": {},
   "source": [
    "If the data cube is created correctly, you will see the variable name (in this case, air temperature) and dimensions for time (365), latitude (720), and longitude (1440). In the next section, we will use this data cube to plot it with Lexcube. But before that, let‚Äôs create a data cube with the climate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd209ab",
   "metadata": {},
   "source": [
    "### Step 5. Create a Data Cube (3D) with Climate Data using `Zarr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3a856",
   "metadata": {},
   "source": [
    "To create the data cube with climate data, we will use the GeoTIFF files from the PRISM climate datasets pre-downloaded for this training. You should have the mean air temperature of the US for July from 2000 to 2025 in GeoTIFF format in your content folder. Before reading the raster files in a loop, we need to create a list of date ranges for those GeoTIFF files, as the GeoTIFF files cover July from 2000 to 2025. Next, we will read each raster file using the rasterio package, convert the raster file to Xarray with latitude and longitude extracted from each file, and create a data cube by appending the 2D data from each raster file. Finally, we‚Äôll set the time in the data cube based on the time range we generated and save it as a Zarr file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ab288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import xarray as xr\n",
    "import os\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "\n",
    "# Create a time list\n",
    "time = pd.date_range(start='2000-07-01', end='2025-07-01', freq='YS-JUL')\n",
    "\n",
    "# Read raster files and create a list of xarray DataArrays\n",
    "data_arrays = []\n",
    "for file in geotiff_files:\n",
    "    with rasterio.open(os.path.join('s3://', s3_bucket_name, file)) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "        height, width = src.shape\n",
    "        y_values = np.arange(height) * src.transform[4] + src.transform[5]\n",
    "        x_values = np.arange(width) * src.transform[0] + src.transform[2]\n",
    "        da = xr.DataArray(data, dims=(\"y\", \"x\"), coords={\"y\": y_values, \"x\": x_values}, name=\"air_temperature\")\n",
    "        data_arrays.append(da)\n",
    "\n",
    "# Combine DataArrays into a single xarray Dataset\n",
    "ds = xr.concat(data_arrays, dim=\"time\")\n",
    "ds[\"time\"] = (\"time\", time)  # Assign month numbers as time coordinates\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db08b18",
   "metadata": {},
   "source": [
    "### Write the climate data cube as a `zarr` datasets to an AWS S3 bucket\n",
    "‚ö†Ô∏è **NOTE: THE FOLLOWING CODE BLOCK SAVES THE DATASET TO S3 AS A ZARR FILE. FOR SECURITY REASONS, WE HAVE REMOVE 'WRITE' ACCESS TO THE PUBLIC BUCKET USED IN THIS TRAINING. YOU CAN USE THIS CODE TO SAVE TO YOUR OWN S3 BUCKET WITHIN TNC's AWS ACCOUNT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 File System\n",
    "s3_out = s3fs.S3FileSystem() # Assumes credentials are set in environment/config\n",
    "\n",
    "# Create an S3Map store for the target location\n",
    "# Replace 'your-bucket-name' and 'path/to/your/file.zarr' with your actual S3 details\n",
    "# s3_uri = 's3://your-bucket-name/path/to/your/dataset.zarr'\n",
    "s3_uri = 's3://ocs-training-2026/advanced/PRISM/climate.zarr'\n",
    "store = s3fs.S3Map(root=s3_uri, s3=s3_out, check=False)\n",
    "\n",
    "# Write the Dataset to the S3 store as a Zarr file\n",
    "# Use mode='w' to create a new store or overwrite an existing one\n",
    "dataset.to_zarr(store=store, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1e2c8",
   "metadata": {},
   "source": [
    "### Read the climate data cube as a `zarr` datasets from an existing AWS S3 bucket\n",
    "Similar to the previous section, we can read the saved Zarr file using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fsspec library is used under the hood to handle the S3 protocol and authentication\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "# Define the S3 URL of the Zarr dataset\n",
    "s3_uri = 's3://ocs-training-2026/advanced/climate.zarr'\n",
    "\n",
    "# Open the dataset with anonymous access for public datasets\n",
    "# Here we are using the open_zarr function from the xarray library, which is a more direct and robust way to open Zarr datasets.\n",
    "ds = xr.open_zarr(\n",
    "    s3_uri,\n",
    "    consolidated=True, # Optional: use if metadata is consolidated for faster opening\n",
    "    storage_options={'anon': True} # Optional: use anonymous access for public datasets\n",
    ")\n",
    "\n",
    "# Alternatively, if you can also use the open_dataset function from the xarray library\n",
    "#ds = xr.open_dataset(\"climate.zarr\", chunks={}, engine=\"zarr\")\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20464cb",
   "metadata": {},
   "source": [
    "If the file is saved correctly, you should see the Xarray details (26 layers in time, 1405 grids on the x-axis, and 621 grids on the y-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d4351",
   "metadata": {},
   "source": [
    "### Step 6. 3D Visualization of `Xarray` by `Lexcube`\n",
    "Let's now use the [lexcube package](https://github.com/msoechting/lexcube) to create some nifty 3D visualizations. Now that we have two Zarr files (one based on random numbers and the second based on climate data), we are ready to plot the 3D visualization of these two data cubes. To have a better and more meaningful 3D plot, let‚Äôs visualize the second data cube (climate data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "# Define the S3 URL of the Zarr dataset\n",
    "s3_uri = 's3://ocs-training-2026/advanced/climate_data.zarr'\n",
    "\n",
    "# Open the dataset with anonymous access for public datasets\n",
    "# Here we are using the open_zarr function from the xarray library, which is a more direct and robust way to open Zarr datasets.\n",
    "ds = xr.open_zarr(\n",
    "    s3_uri,\n",
    "    consolidated=True, # Optional: use if metadata is consolidated for faster opening\n",
    "    storage_options={'anon': True} # Optional: use anonymous access for public datasets\n",
    ")\n",
    "\n",
    "da = ds[\"air_temperature\"][:,:,:]\n",
    "\n",
    "##### This may fail in certain testing environments due to missing notebook widgets\n",
    "import lexcube \n",
    "w = lexcube.Cube3DWidget(da,cmap=\"RdYlBu_r\", vmin=0, vmax=30)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d4b5b",
   "metadata": {},
   "source": [
    "### BONUS Exercise: Let‚Äôs zoom in on a given area and look at the mean air temperature for July 2025 displayed on the top layer. Additionally, by hovering your mouse over the latitude and longitude axes, you can see the air temperature for different years. Which year was the warmest? At which coordinates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb143ca8",
   "metadata": {},
   "source": [
    "### Step 7. What Else Can We Do with Lexcube?\n",
    "Let‚Äôs assume you want to clip this plot for a specific location (latitude and longitude) and a specific time. Instead of manually zooming in and out on each axis, which is not very convenient, you can activate the slider by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a22a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.show_sliders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092631fc",
   "metadata": {},
   "source": [
    "with that slider, you can clip your plot for any location and timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ac12d",
   "metadata": {},
   "source": [
    "And last but not least, if you want to save the plot in your local folder, you can do so by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.savefig(fname=\"climate.png\", include_ui=True, dpi_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8270c44",
   "metadata": {},
   "source": [
    "The PNG file will be saved in your folder. If you want to change the color ramp, Lexcube supports many color maps that you can find in the GitHub repository mentioned in the reference section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad83eb",
   "metadata": {},
   "source": [
    "## üìö References and Additional Resources\n",
    "M., S√∂chting, M. D., Mahecha, D., Montero, and G., Scheuermann, Lexcube: Interactive Visualization of Large Earth System Data Cubes (2023). IEEE Computer Graphics and Applications. doi:10.1109/MCG.2023.3321989.\n",
    "\n",
    "[https://github.com/msoechting/lexcube](https://github.com/msoechting/lexcube)\n",
    "\n",
    "[https://pubmed.ncbi.nlm.nih.gov/37812545/](https://pubmed.ncbi.nlm.nih.gov/37812545/)\n",
    "\n",
    "[https://eo4society.esa.int/2022/05/25/exploring-earth-system-data-with-lexcube/](https://eo4society.esa.int/2022/05/25/exploring-earth-system-data-with-lexcube/)\n",
    "\n",
    "[https://www.linkedin.com/posts/miguel-mahecha-625548197_lexcube-activity-7156200204700966913-5lOb?utm_source=share&utm_medium=member_desktop](https://www.linkedin.com/posts/miguel-mahecha-625548197_lexcube-activity-7156200204700966913-5lOb?utm_source=share&utm_medium=member_desktop)\n",
    "\n",
    "PRISM Climate Group, Oregon State University, [https://prism.oregonstate.edu](https://prism.oregonstate.edu), date created 1981‚Äì2022, accessed 19 Dec 2024."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
